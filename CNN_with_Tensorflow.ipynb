{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**A  ConvNet to identify sign language digits using the TF Keras Functional API**\n",
        "\n",
        "**After this assignment you will be able to:**\n",
        "\n",
        "-Build and train a ConvNet in TensorFlow for a binary classification problem\n",
        "\n",
        "-Build and train a ConvNet in TensorFlow for a multiclass classification problem\n",
        "\n",
        "-Explain different use cases for the Sequential and Functional APIs"
      ],
      "metadata": {
        "id": "ZMCykfxsQbEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_EqrcC9VQFkM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imread\n",
        "import scipy\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.python.framework import ops\n",
        "from cnn_utils import *\n",
        "from cnn_utils import load_happy_dataset\n",
        "from test_utils import summary, comparator\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Data and Split the Data into Train/Test Sets\n",
        "#X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset()\n",
        "\n",
        "train_dataset = h5py.File('/content/train_happy.h5', \"r\")\n",
        "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "test_dataset = h5py.File('/content/test_happy.h5', \"r\")\n",
        "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "\n",
        "train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "X_train_orig = train_set_x_orig\n",
        "Y_train_orig = train_set_y_orig\n",
        "\n",
        "X_test_orig = test_set_x_orig\n",
        "Y_test_orig = test_set_y_orig\n",
        "# Normalize image vectors\n",
        "X_train = X_train_orig/255.\n",
        "X_test = X_test_orig/255.\n",
        "\n",
        "# Reshape\n",
        "Y_train = Y_train_orig.T\n",
        "Y_test = Y_test_orig.T\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDbIEmnUROOb",
        "outputId": "7747186e-f11b-4533-ac44-4b4d8eb82adc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples = 600\n",
            "number of test examples = 150\n",
            "X_train shape: (600, 64, 64, 3)\n",
            "Y_train shape: (600, 1)\n",
            "X_test shape: (150, 64, 64, 3)\n",
            "Y_test shape: (150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 124\n",
        "plt.imshow(X_train_orig[index]) #display sample training image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k6-YDZuvoa3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HappyModel**\n",
        "\n",
        "Implement the happyModel function below to build the following model: ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE(fully connected)."
      ],
      "metadata": {
        "id": "d1egDOZcq9ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: happyModel\n",
        "\n",
        "def happyModel():\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the binary classification model:\n",
        "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
        "\n",
        "    Note that for simplicity and grading purposes, you'll hard-code all the values\n",
        "    such as the stride and kernel (filter) sizes.\n",
        "    Normally, functions should take these values as function parameters.\n",
        "\n",
        "    Arguments:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    model -- TF Keras model (object containing the information for the entire training process)\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.Input(shape=(64 , 64 ,3)),\n",
        "            ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3\n",
        "            tfl.ZeroPadding2D(padding=3), # As import tensorflow.keras.layers as tfl\n",
        "            ## Conv2D with 32 7x7 filters and stride of 1\n",
        "            tfl.Conv2D(filters=32,kernel_size=7,strides=1),\n",
        "            ## BatchNormalization for axis 3\n",
        "            tfl.BatchNormalization(axis=3, momentum=0.99, epsilon=0.001),\n",
        "            ## ReLU\n",
        "            tfl.ReLU(),\n",
        "            ## Max Pooling 2D with default parameters\n",
        "            tfl.MaxPool2D(),\n",
        "            ## Flatten layer\n",
        "            tfl.Flatten(),\n",
        "            ## Dense layer with 1 unit for output & 'sigmoid' activation\n",
        "            tfl.Dense(1,activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "pqd1rQjwrBg8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "happy_model = happyModel()\n",
        "# Print a summary for each layer\n",
        "for layer in summary(happy_model):\n",
        "    print(layer)\n",
        "\n",
        "output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],\n",
        "            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],\n",
        "            ['BatchNormalization', (None, 64, 64, 32), 128],\n",
        "            ['ReLU', (None, 64, 64, 32), 0],\n",
        "            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],\n",
        "            ['Flatten', (None, 32768), 0],\n",
        "            ['Dense', (None, 1), 32769, 'sigmoid']]\n",
        "\n",
        "comparator(summary(happy_model), output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhmlLnxhsrTj",
        "outputId": "a6862f2b-c0a5-4160-e63e-aa25ff2add3f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))]\n",
            "['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform']\n",
            "['BatchNormalization', (None, 64, 64, 32), 128]\n",
            "['ReLU', (None, 64, 64, 32), 0]\n",
            "['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid']\n",
            "['Flatten', (None, 32768), 0]\n",
            "['Dense', (None, 1), 32769, 'sigmoid']\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "happy_model.compile(optimizer='adam',\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lBVrTbJfuVUG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Evaluate the Model**"
      ],
      "metadata": {
        "id": "WMyhZzTyuixU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "happy_model.fit(X_train, Y_train, epochs=10, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-8SEy_2umoS",
        "outputId": "fae15d42-6c0b-44a1-952f-b1208dc0103d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 6s 102ms/step - loss: 0.6312 - accuracy: 0.7850\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 5s 139ms/step - loss: 0.2831 - accuracy: 0.8950\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2068 - accuracy: 0.9117\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.1271 - accuracy: 0.9533\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 5s 138ms/step - loss: 0.1563 - accuracy: 0.9433\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1487 - accuracy: 0.9500\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2511 - accuracy: 0.9183\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 5s 139ms/step - loss: 0.1570 - accuracy: 0.9450\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.0949 - accuracy: 0.9683\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.1220 - accuracy: 0.9567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cb3c016cbb0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "happy_model.evaluate(X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pW9yhdmvIXC",
        "outputId": "88d1c13a-75b7-4dc5-9b37-94cbe45fc427"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 80ms/step - loss: 0.1305 - accuracy: 0.9667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.130497545003891, 0.9666666388511658]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Easy, right? But what if you need to build a model with shared layers, branches, or multiple inputs and outputs? This is where Sequential, with its beautifully simple yet limited functionality, won't be able to help you.\n",
        "\n",
        "Next up: Enter the Functional API, your slightly more complex, highly flexible friend."
      ],
      "metadata": {
        "id": "NdlcJdQ-vYnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Functional API**\n",
        "\n",
        "Welcome to the second half of the assignment, where you'll use Keras' flexible Functional API to build a ConvNet that can differentiate between 6 sign language digits.\n",
        "\n",
        "The Functional API can handle models with non-linear topology, shared layers, as well as layers with multiple inputs or outputs. Imagine that, where the Sequential API requires the model to move in a linear fashion through its layers, the Functional API allows much more flexibility. Where Sequential is a straight line, a Functional model is a graph, where the nodes of the layers can connect in many more ways than one."
      ],
      "metadata": {
        "id": "Szfm2Ez3vgpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the SIGNS Dataset**"
      ],
      "metadata": {
        "id": "GZtfISg3wlWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data (signs)\n",
        "#X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()\n",
        "\n",
        "train_dataset = h5py.File('/content/train_signs.h5', \"r\")\n",
        "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "test_dataset = h5py.File('/content/test_signs.h5', \"r\")\n",
        "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "\n",
        "train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "X_train_orig = train_set_x_orig\n",
        "Y_train_orig = train_set_y_orig\n",
        "\n",
        "X_test_orig = test_set_x_orig\n",
        "Y_test_orig = test_set_y_orig"
      ],
      "metadata": {
        "id": "ega5md8Mva-V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an image from the dataset\n",
        "index = 9\n",
        "plt.imshow(X_train_orig[index])\n",
        "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
      ],
      "metadata": {
        "id": "I6TkS71mzEn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the Data into Train/Test Sets**"
      ],
      "metadata": {
        "id": "JSpclPhlzNLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train_orig/255.\n",
        "X_test = X_test_orig/255.\n",
        "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
        "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "metadata": {
        "id": "AM2n0MOtzIc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Propagation**"
      ],
      "metadata": {
        "id": "KkXSXSBKziEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In TensorFlow, there are built-in functions that implement the convolution steps for you. By now, you should be familiar with how TensorFlow builds computational graphs. In the Functional API, you create a graph of layers. This is what allows such great flexibility.\n",
        "\n",
        "However, the following model could also be defined using the Sequential API since the information flow is on a single line. But don't deviate. What we want you to learn is to use the functional API."
      ],
      "metadata": {
        "id": "wgFjbrxS04sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2 - convolutional_model**\n",
        "\n",
        "Implement the convolutional_model function below to build the following model: CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE.\n"
      ],
      "metadata": {
        "id": "qoT6A3ww6aqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: convolutional_model\n",
        "\n",
        "def convolutional_model(input_shape):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model:\n",
        "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
        "\n",
        "    Note that for simplicity and grading purposes, you'll hard-code some values\n",
        "    such as the stride and kernel (filter) sizes.\n",
        "    Normally, functions should take these values as function parameters.\n",
        "\n",
        "    Arguments:\n",
        "    input_img -- input dataset, of shape (input_shape)\n",
        "\n",
        "    Returns:\n",
        "    model -- TF Keras model (object containing the information for the entire training process)\n",
        "    \"\"\"\n",
        "\n",
        "    input_img = tf.keras.Input(shape=input_shape)\n",
        "    ## CONV2D: 8 filters 4x4, stride of 1, padding 'SAME'\n",
        "    Z1 = tfl.Conv2D(filters= 8. , kernel_size=4 , padding='same',strides=1)(input_img)\n",
        "    ## RELU\n",
        "    A1 = tfl.ReLU()(Z1)\n",
        "    ## MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
        "    P1 = tfl.MaxPool2D(pool_size=8, strides=8, padding='SAME')(A1)\n",
        "    ## CONV2D: 16 filters 2x2, stride 1, padding 'SAME'\n",
        "    Z2 = tfl.Conv2D(filters= 16. , kernel_size=2 , padding='same',strides=1)(P1)\n",
        "    ## RELU\n",
        "    A2 =  tfl.ReLU()(Z2)\n",
        "    ## MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
        "    P2 = tfl.MaxPool2D(pool_size=4, strides=4, padding='SAME')(A2)\n",
        "    ## FLATTEN\n",
        "    F = tfl.Flatten()(P2)\n",
        "    ## Dense layer\n",
        "    ## 6 neurons in output layer. Hint: one of the arguments should be \"activation='softmax'\"\n",
        "    outputs = tfl.Dense(units= 6 , activation='softmax')(F)\n",
        "    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "O0swjFOYzjy5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model = convolutional_model((64, 64, 3))\n",
        "conv_model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "conv_model.summary()\n",
        "\n",
        "output = [['InputLayer', [(None, 64, 64, 3)], 0],\n",
        "        ['Conv2D', (None, 64, 64, 8), 392, 'same', 'linear', 'GlorotUniform'],\n",
        "        ['ReLU', (None, 64, 64, 8), 0],\n",
        "        ['MaxPooling2D', (None, 8, 8, 8), 0, (8, 8), (8, 8), 'same'],\n",
        "        ['Conv2D', (None, 8, 8, 16), 528, 'same', 'linear', 'GlorotUniform'],\n",
        "        ['ReLU', (None, 8, 8, 16), 0],\n",
        "        ['MaxPooling2D', (None, 2, 2, 16), 0, (4, 4), (4, 4), 'same'],\n",
        "        ['Flatten', (None, 64), 0],\n",
        "        ['Dense', (None, 6), 390, 'softmax']]\n",
        "\n",
        "comparator(summary(conv_model), output)"
      ],
      "metadata": {
        "id": "ZP2edTZj7Bs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Both the Sequential and Functional APIs return a TF Keras model object. The only difference is how inputs are handled inside the object model!"
      ],
      "metadata": {
        "id": "_rGxxVtk7XIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.4 - Train the Model**"
      ],
      "metadata": {
        "id": "J2k8MFTO8Du2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(64)\n",
        "history = conv_model.fit(train_dataset, epochs=100, validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "E40Cz2nc7aOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5 - History Object**\n",
        "\n",
        "The history object is an output of the .fit() operation, and provides a record of all the loss and metric values in memory. It's stored as a dictionary that you can retrieve at history.history:"
      ],
      "metadata": {
        "id": "mvoFAENM9UP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "YfFMk-i39epM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now visualize the loss over time using history.history:"
      ],
      "metadata": {
        "id": "v1aweaec9uL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n",
        "# model was trained on.\n",
        "df_loss_acc = pd.DataFrame(history.history)\n",
        "df_loss= df_loss_acc[['loss','val_loss']]\n",
        "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
        "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
        "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
        "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
        "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')"
      ],
      "metadata": {
        "id": "ewf0DNqC9vym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations!\n",
        "You've finished the assignment and built two models: One that recognizes smiles, and another that recognizes SIGN language with almost 80% accuracy on the test set. In addition to that, you now also understand the applications of two Keras APIs: Sequential and Functional. Nicely done!"
      ],
      "metadata": {
        "id": "qdEpiigF-Xyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DqcKP2uC-abh"
      }
    }
  ]
}